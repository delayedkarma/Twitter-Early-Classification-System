{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter early warning system\n",
    "\n",
    "### Every so often Justin Bieber brushes up against the authorities or media outlets, whether it’s for drunk driving or objectionable videos. Can you design and implement an early warning system based on Twitter feed that would alert us when The Bieber is in trouble?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the statement of the problem seemed simple enough:: given a tweet from the twitter feed, can it be flagged if, according to the tweet, Justin Bieber is in trouble. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building the Bieber -- Creating the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample given to me comprised 20 sample tweets, so not nearly enough to train a model on. This meant that I had to figure out a way to pull tweets from the twitter feed. In this section, I roughly describe the process of data extraction. I used the Tweepy package (http://www.tweepy.org/) and learned how to use Tweepy from various blogs on the internet, such as https://marcobonzanini.com/2015/03/02/mining-twitter-data-with-python-part-1/. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step involved acquiring the Consumer Key, Consumer Secret, Access Token and Access Token Secret, in order to be able to access the Twitter stream. I put the 4 keys in a file __credentials.py__ so that I could just import the key values each time I had to access the twitter feed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roughly, credentials.py contains::\n",
    "\n",
    "CONSUMER_KEY = ''\n",
    "CONSUMER_SECRET = ''\n",
    "\n",
    "\n",
    "ACCESS_TOKEN = ''\n",
    "ACCESS_SECRET = ''\n",
    "\n",
    "# With the appropriate key values filled in of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the relevant packages for accessing and handling twitter data\n",
    "\n",
    "import tweepy           \n",
    "import pandas as pd     \n",
    "import numpy as np      \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from credentials import *\n",
    "\n",
    "# The following function defines the api call\n",
    "\n",
    "def api_call():\n",
    "    \n",
    "    # Keys\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(ACCESS_TOKEN, ACCESS_SECRET)\n",
    "\n",
    "    # Return the API:\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function defines a routine by which we can use tweepy to extract tweets from the \n",
    "# Twitter feed using keywords and number of desired queries\n",
    "\n",
    "def twitter_pull(query,num_tweets):\n",
    "    \n",
    "    api = api_call()\n",
    "    tweets = [status for status in tweepy.Cursor(api.search, q=query, lang='en').items(num_tweets)]\n",
    "    return(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sample tweets provided, we run the twitter_pull routine multiple times for various combinations of keywords, such as:\n",
    "* 'justin + bieber' -- Investigate all real-time Justin Bieber related tweets for a pattern\n",
    "* 'twitter' -- Pull any tweets (because the data file for all tweets contains the word 'twitter')\n",
    "* 'justin + bieber + drunk'\n",
    "* 'justin + bieber + jail'\n",
    "* 'justin + bieber + reckless'\n",
    "etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of the above process is shown below::\n",
    "\n",
    "tweets_sample = twitter_pull('justin + bieber + police',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function converts all the tweets to a dataframe\n",
    "\n",
    "def to_df(tweet_list):\n",
    "    \n",
    "    df = pd.DataFrame(data=[tweet.text for tweet in tweet_list],columns=['Tweet_text'])\n",
    "    \n",
    "    df['Id'] = [tweet.id for tweet in tweet_list]\n",
    "    \n",
    "    df['Date'] = [tweet.created_at for tweet in tweet_list]\n",
    "    \n",
    "    df['Likes'] = [tweet.favorite_count for tweet in tweet_list]\n",
    "    \n",
    "    df['Retweets'] = [tweet.retweet_count for tweet in tweet_list]\n",
    "    \n",
    "    df['Trouble?'] = False # Set False by default\n",
    "    \n",
    "    df.drop_duplicates(['Tweet_text'],inplace=True) # Drop duplicate tweet_text values\n",
    "    \n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Trouble?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Hart, Dutch Police During, Democrats Get...</td>\n",
       "      <td>963203755704569856</td>\n",
       "      <td>2018-02-13 00:10:45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @abcWNN: BREAKING: Witnesses say Justin Bie...</td>\n",
       "      <td>962928190078373888</td>\n",
       "      <td>2018-02-12 05:55:45</td>\n",
       "      <td>0</td>\n",
       "      <td>9360</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @halfneIson: #imagine Ryan Gosling chopping...</td>\n",
       "      <td>962776747145093120</td>\n",
       "      <td>2018-02-11 19:53:58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @feryooit: A USED CAR! THE HYUNDAI RETWEET ...</td>\n",
       "      <td>962756514074128384</td>\n",
       "      <td>2018-02-11 18:33:34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A USED CAR! THE HYUNDAI RETWEET TO WIN DIARRHE...</td>\n",
       "      <td>962731041969512448</td>\n",
       "      <td>2018-02-11 16:52:21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>“I, too, had the morning to myself. Had poache...</td>\n",
       "      <td>962472157199544320</td>\n",
       "      <td>2018-02-10 23:43:38</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @HaileywWelch: I use to listen to Justin Bi...</td>\n",
       "      <td>962207688754114560</td>\n",
       "      <td>2018-02-10 06:12:44</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @whitebison66: In honor (?) of #Juchefest: ...</td>\n",
       "      <td>962200912218353665</td>\n",
       "      <td>2018-02-10 05:45:48</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>In honor (?) of #Juchefest: #AddJucheToASongTi...</td>\n",
       "      <td>962198456470200320</td>\n",
       "      <td>2018-02-10 05:36:03</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I use to listen to Justin Bieber &amp;amp; fifth h...</td>\n",
       "      <td>962177195870445568</td>\n",
       "      <td>2018-02-10 04:11:34</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Tweet_text                  Id  \\\n",
       "0  David Hart, Dutch Police During, Democrats Get...  963203755704569856   \n",
       "1  RT @abcWNN: BREAKING: Witnesses say Justin Bie...  962928190078373888   \n",
       "2  RT @halfneIson: #imagine Ryan Gosling chopping...  962776747145093120   \n",
       "3  RT @feryooit: A USED CAR! THE HYUNDAI RETWEET ...  962756514074128384   \n",
       "4  A USED CAR! THE HYUNDAI RETWEET TO WIN DIARRHE...  962731041969512448   \n",
       "5  “I, too, had the morning to myself. Had poache...  962472157199544320   \n",
       "6  RT @HaileywWelch: I use to listen to Justin Bi...  962207688754114560   \n",
       "7  RT @whitebison66: In honor (?) of #Juchefest: ...  962200912218353665   \n",
       "8  In honor (?) of #Juchefest: #AddJucheToASongTi...  962198456470200320   \n",
       "9  I use to listen to Justin Bieber &amp; fifth h...  962177195870445568   \n",
       "\n",
       "                 Date  Likes  Retweets  Trouble?  \n",
       "0 2018-02-13 00:10:45      0         0     False  \n",
       "1 2018-02-12 05:55:45      0      9360     False  \n",
       "2 2018-02-11 19:53:58      0         1     False  \n",
       "3 2018-02-11 18:33:34      0         1     False  \n",
       "4 2018-02-11 16:52:21      0         1     False  \n",
       "5 2018-02-10 23:43:38      0         0     False  \n",
       "6 2018-02-10 06:12:44      0         2     False  \n",
       "7 2018-02-10 05:45:48      0         1     False  \n",
       "8 2018-02-10 05:36:03      1         1     False  \n",
       "9 2018-02-10 04:11:34      8         2     False  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = to_df(tweets_sample)\n",
    "df_sample.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the __Trouble?__ column in the dataframe to be __False__, under the assumption that most of the tweets encountered on the twitter feed will not involve Justin Bieber in trouble. \n",
    "\n",
    "We then investigate the text of each of the tweets to see whether the __Trouble?__ flag should be changed to __True__\n",
    "or not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  David Hart, Dutch Police During, Democrats Get In Justin Bieber\n",
      "\n",
      "\n",
      "1 :  RT @abcWNN: BREAKING: Witnesses say Justin Bieber hit a photographer in Beverly Hills on Wednesday evening. Police say it appears to be an…\n",
      "\n",
      "\n",
      "2 :  RT @halfneIson: #imagine Ryan Gosling chopping your copy of Justin Bieber’s Never Say Never in 3D in half with his bare hand. You call the…\n",
      "\n",
      "\n",
      "3 :  RT @feryooit: A USED CAR! THE HYUNDAI RETWEET TO WIN DIARRHEA FART NIPPO REPORT CARD: HOLIDAYS 42, RUSSIA MY BROTHER LIKES TO NAIL. HEALTH…\n",
      "\n",
      "\n",
      "4 :  A USED CAR! THE HYUNDAI RETWEET TO WIN DIARRHEA FART NIPPO REPORT CARD: HOLIDAYS 42, RUSSIA MY BROTHER LIKES TO NAI… https://t.co/ANJXw5bUdl\n",
      "\n",
      "\n",
      "5 :  “I, too, had the morning to myself. Had poached eggs on toast at home, visited an ATM, took a spin class, just in c… https://t.co/rHLecVPOt6\n",
      "\n",
      "\n",
      "6 :  RT @HaileywWelch: I use to listen to Justin Bieber &amp; fifth harmony... then @the_emilywelch showed me REAL artists like Janis Joplin, eagles…\n",
      "\n",
      "\n",
      "7 :  RT @whitebison66: In honor (?) of #Juchefest: #AddJucheToASongTitle\n",
      "Too Much Juche in the Pants\n",
      "Juche (Rebecca Black) \n",
      "Gimme Juche\n",
      "Juche (J…\n",
      "\n",
      "\n",
      "8 :  In honor (?) of #Juchefest: #AddJucheToASongTitle\n",
      "Too Much Juche in the Pants\n",
      "Juche (Rebecca Black) \n",
      "Gimme Juche\n",
      "Ju… https://t.co/W5Cavzek60\n",
      "\n",
      "\n",
      "9 :  I use to listen to Justin Bieber &amp; fifth harmony... then @the_emilywelch showed me REAL artists like Janis Joplin,… https://t.co/5OHdfrSZ0i\n",
      "\n",
      "\n",
      "10 :  RT @feryooit: MAKE MONEY RETWEET TO WIN CAT DIARRHEA AFTERYEARS AFRICAN HOUSE POKEMON GIVEAWAY FIFA COW MINDCRACK WHORESON JAZZ SOME RANDOM…\n",
      "\n",
      "\n",
      "11 :  MAKE MONEY RETWEET TO WIN CAT DIARRHEA AFTERYEARS AFRICAN HOUSE POKEMON GIVEAWAY FIFA COW MINDCRACK WHORESON JAZZ S… https://t.co/6oIPid4Say\n",
      "\n",
      "\n",
      "12 :  @FandomSegreti One Direction.. ( non te le sto nemmeno a dire perché sono davvero troppe😂 ) Charlie Puth... lo ador… https://t.co/qMjNjOgWQO\n",
      "\n",
      "\n",
      "13 :  'Cocky' Justin Bieber tested positive for pot, Xanax, police docs say - https://t.co/nydzreozEc - https://t.co/Zy8UMOLcyV\n",
      "\n",
      "\n",
      "14 :  \"I too had the morning to myself, had poached eggs on toast at home, visited an ATM, took a spin class...just in ca… https://t.co/Rl2y8MgjNs\n",
      "\n",
      "\n",
      "15 :  lol I had a dream I jumped into this big car on the road with my friend bc running from police for some reason and… https://t.co/cKSLlAmG5s\n",
      "\n",
      "\n",
      "16 :  @IXVIIVIIIIV @gigandect Two police man try to get a troubled Japanese youth selling bootleg Justin Bieber albums off the street.\n",
      "\n",
      "\n",
      "17 :  Justin Bieber once killed 142 Columbian drug lords using a single magazine. He then proceeded to sit down and read as police arrived.\n",
      "\n",
      "\n",
      "18 :  Setting up a man's life based on hit pieces, especially now that he's \"neo nazi\". This is insanity. The Pentagon's… https://t.co/mw09CloGVM\n",
      "\n",
      "\n",
      "19 :  Three most important times in our countries history: malice at the police, Justin Bieber mocking that reporters lau… https://t.co/XBbylRqxHh\n",
      "\n",
      "\n",
      "20 :  RT @JBCrewdetcom: Justin Bieber was involved in a car crash that happened late tonight. Police are still under investigation to determine i…\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Go through tweets, and identify problematic ones, rest will remain 'False' for 'Trouble?'\n",
    "for i in range(len(df_sample)):\n",
    "    print(i,\": \", df_sample['Tweet_text'][i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Trouble?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @abcWNN: BREAKING: Witnesses say Justin Bie...</td>\n",
       "      <td>962928190078373888</td>\n",
       "      <td>2018-02-12 05:55:45</td>\n",
       "      <td>0</td>\n",
       "      <td>9360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'Cocky' Justin Bieber tested positive for pot,...</td>\n",
       "      <td>961481959426969601</td>\n",
       "      <td>2018-02-08 06:08:57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>RT @JBCrewdetcom: Justin Bieber was involved i...</td>\n",
       "      <td>959941966309085186</td>\n",
       "      <td>2018-02-04 00:09:34</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Tweet_text                  Id  \\\n",
       "1   RT @abcWNN: BREAKING: Witnesses say Justin Bie...  962928190078373888   \n",
       "13  'Cocky' Justin Bieber tested positive for pot,...  961481959426969601   \n",
       "20  RT @JBCrewdetcom: Justin Bieber was involved i...  959941966309085186   \n",
       "\n",
       "                  Date  Likes  Retweets  Trouble?  \n",
       "1  2018-02-12 05:55:45      0      9360      True  \n",
       "13 2018-02-08 06:08:57      0         0      True  \n",
       "20 2018-02-04 00:09:34      0        54      True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Based on our judgement, in the above list of tweets, tweets with indices 1, 13 and 20 indicate trouble for Bieber\n",
    "# 19 is ambiguous, so we won't flag it True for now\n",
    "\n",
    "trouble_list_sample = [1,13,20]\n",
    "for index in trouble_list_sample:\n",
    "    df_sample.loc[index,'Trouble?'] = True\n",
    "df_sample[df_sample['Trouble?']==True]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We repeat the above process several times and concatenate the resultant data frames to finally (after duplicate removal) obtain a dataset with 986 entries. The data is stored in a json file which can be accessed later. The tweet texts are cleaned using the preprocessor package (https://pypi.python.org/pypi/tweet-preprocessor/0.4.0) and made lower-case (Though the Count Vectorizer would have extracted tokens using regular expressions anyway). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Id</th>\n",
       "      <th>Date</th>\n",
       "      <th>Likes</th>\n",
       "      <th>Retweets</th>\n",
       "      <th>Trouble?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>david hart, dutch police during, democrats get...</td>\n",
       "      <td>963203755704569856</td>\n",
       "      <td>2018-02-13 00:10:45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>: breaking: witnesses say justin bieber hit a ...</td>\n",
       "      <td>962928190078373888</td>\n",
       "      <td>2018-02-12 05:55:45</td>\n",
       "      <td>0</td>\n",
       "      <td>9360</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>: ryan gosling chopping your copy of justin bi...</td>\n",
       "      <td>962776747145093120</td>\n",
       "      <td>2018-02-11 19:53:58</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>: a used car! the hyundai retweet to win diarr...</td>\n",
       "      <td>962756514074128384</td>\n",
       "      <td>2018-02-11 18:33:34</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a used car! the hyundai retweet to win diarrhe...</td>\n",
       "      <td>962731041969512448</td>\n",
       "      <td>2018-02-11 16:52:21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Tweet_text                  Id  \\\n",
       "0  david hart, dutch police during, democrats get...  963203755704569856   \n",
       "1  : breaking: witnesses say justin bieber hit a ...  962928190078373888   \n",
       "2  : ryan gosling chopping your copy of justin bi...  962776747145093120   \n",
       "3  : a used car! the hyundai retweet to win diarr...  962756514074128384   \n",
       "4  a used car! the hyundai retweet to win diarrhe...  962731041969512448   \n",
       "\n",
       "                 Date  Likes  Retweets  Trouble?  \n",
       "0 2018-02-13 00:10:45      0         0     False  \n",
       "1 2018-02-12 05:55:45      0      9360      True  \n",
       "2 2018-02-11 19:53:58      0         1     False  \n",
       "3 2018-02-11 18:33:34      0         1     False  \n",
       "4 2018-02-11 16:52:21      0         1     False  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as pre\n",
    "df_sample['Tweet_text'] = df_sample['Tweet_text'].map(lambda x: pre.clean(x))\n",
    "df_sample['Tweet_text'] = df_sample['Tweet_text'].map(lambda x: x.lower())\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is stored in a json file with 986 entries, with 935 False values and 51 True values. We sample 500 False values at random, as well as choose the first 50 True value entries in the dataset, and merge them to create a dataset where the ratio of False:True classes (Not in trouble: Is in trouble) is 10:1, with the False and True targets appropriately shuffled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Trouble?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>: check out justin bieber’s recent instagram a...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>: justin bieber via instagram stories:</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>: please rt this and reply your favorite song ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>: lucas was only when he outdid justin bieber</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>oh my god we're listening to justin bieber - o...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id                                         Tweet_text  Trouble?\n",
       "0   0  : check out justin bieber’s recent instagram a...     False\n",
       "1   1             : justin bieber via instagram stories:     False\n",
       "2   2  : please rt this and reply your favorite song ...     False\n",
       "3   3      : lucas was only when he outdid justin bieber     False\n",
       "4   4  oh my god we're listening to justin bieber - o...     False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assume the dataset with 986 entries is df_trial_1\n",
    "\n",
    "df_trial_1 = pd.read_json('bieber_df_trial_1.json',orient='frame')\n",
    "df_trial_1.reset_index(inplace=True,drop=True)\n",
    "\n",
    "df_trial_1.reset_index(inplace=True)\n",
    "df_trial_1.columns = ['Id','Trouble?','Tweet_text']\n",
    "\n",
    "df_trial_1 = df_trial_1[['Id','Tweet_text','Trouble?']]\n",
    "df_trial_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_indlist = df_trial_1[df_trial_1['Trouble?']==True].index.tolist()\n",
    "false_indlist = df_trial_1[df_trial_1['Trouble?']==False].index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "true_indlist = true_indlist[:50]\n",
    "\n",
    "rand_false_ind = []\n",
    "count=0\n",
    "for i in false_indlist:\n",
    "        rand_false_ind.append(random.choice(false_indlist))\n",
    "        count+=1\n",
    "        if count==500:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    500\n",
       "True      50\n",
       "Name: Trouble?, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "false_and_true_list = rand_false_ind + true_indlist\n",
    "df_select = df_trial_1.iloc[false_and_true_list]\n",
    "\n",
    "df_select['Trouble?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_select = df_select.sample(frac=1).reset_index(drop=True) # Shuffles the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataframe is then stored as another json file (bieber_train_set_1.json) which can then be used for predictive modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. How is the Bieber? -- Training the early warning system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Tweet_text</th>\n",
       "      <th>Trouble?</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>897</td>\n",
       "      <td>: star eecting a girl considering hypobirthing …</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>452</td>\n",
       "      <td>: i was up far too late doing this.</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>954</td>\n",
       "      <td>justin bieber - fast car (tracy chapman) hour!...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>707</td>\n",
       "      <td>: next time justin bieber is on tour i'm fucki...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>193</td>\n",
       "      <td>: tweet to appreciate justin bieber smiling</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Id                                         Tweet_text  Trouble?\n",
       "0  897   : star eecting a girl considering hypobirthing …     False\n",
       "1  452                : i was up far too late doing this.     False\n",
       "2  954  justin bieber - fast car (tracy chapman) hour!...     False\n",
       "3  707  : next time justin bieber is on tour i'm fucki...     False\n",
       "4  193        : tweet to appreciate justin bieber smiling     False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's load in the dataset that we created\n",
    "\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, roc_auc_score, \\\n",
    "confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Read in the dataset -- 550 rows.. 10:1 False:True\n",
    "\n",
    "df = pd.read_json('bieber_train_set_1.json',orient='frame')\n",
    "df.reset_index(inplace=True,drop=True)\n",
    "df = df[['Id','Tweet_text','Trouble?']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    500\n",
       "1     50\n",
       "Name: Trouble?, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Map the False:True to 0:1 for ease of representation\n",
    "\n",
    "df['Trouble?'] = df['Trouble?'].map({False:0,True:1})\n",
    "df['Trouble?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline estimate, we use a __Logistic Regression__ classifier on our data, varying the regularization parameter __C__ via a Grid Search, since the data is sparse and high dimensional (https://books.google.com/books?hl=en&lr=&id=1-4lDQAAQBAJ&oi=fnd&pg=PP1&dq=a.+mueller+python+book&ots=27hUHQPMWY&sig=4xUBGsm7GasXj10yxfW01kqshIU#v=onepage&q&f=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To represent the tweet texts for our Machine Learning task, we use the __Bag of Words__ representation (https://en.wikipedia.org/wiki/Bag-of-words_model), which involves Tokenization of the text (into features), buildiung the vocabulary of all the words that appear in the twitter text in our dataset, and counting the frequency of each of these words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping in mind that the dataset is not balanced, we try a Logistic Regression classifier to estimate the accuracy of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a 70:30 split for train and test sets\n",
    "\n",
    "train = df[:385]\n",
    "test = df[385:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10951008645533142"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Trouble?'].value_counts()[1]/train['Trouble?'].value_counts()[0]\n",
    "# 11% of the training set has a positive target value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0784313725490196"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Trouble?'].value_counts()[1]/test['Trouble?'].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_train, y_train = train['Tweet_text'], train['Trouble?']\n",
    "text_test, y_test = test['Tweet_text'], test['Trouble?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 1526)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "# So the train set has 1526 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.9404748878433089\n"
     ]
    }
   ],
   "source": [
    "# Let's evaluate the cross-validation score of the model:: \n",
    "\n",
    "cv_scores = cross_val_score(LogisticRegression(),X_train,y_train,cv=10)\n",
    "\n",
    "print(\"Mean cross-validation accuracy: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for the training set, even a random guess of 0 for the target variable should be right ~89% of the time. Our accuracy of ~94% is higher than that, but we should probably look at other metrics as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9454545454545454\n",
      "Best parameters:  {'C': 100}\n"
     ]
    }
   ],
   "source": [
    "# Let's also do a Grid Search over the regularization parameter C\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10,100,1000]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the cross-val accuracy increases marginally if we tune the __C__ parameter. How does our model generalize to the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(lr_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using this basic Logistic Regression classifier method, without addressing the class imbalance in the dataset, we achieve a cross-validation accuracy of ~96% on the test set.\n",
    "\n",
    "Let's set the minimum document frequency to 3, and exclude stopwords, to see if we can improve the generalization performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 338)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=3).fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "X_train.shape # Reduces the number of features to 1/3 of the original number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean cross-validation accuracy: 0.9429003902688115\n"
     ]
    }
   ],
   "source": [
    "cv_scores = cross_val_score(LogisticRegression(),X_train,y_train,cv=10)\n",
    "\n",
    "print(\"Mean cross-validation accuracy: {}\".format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9428571428571428\n",
      "Best parameters:  {'C': 1}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10,100,1000]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(lr_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(385, 235)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(min_df=3, stop_words=\"english\").fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9376623376623376\n"
     ]
    }
   ],
   "source": [
    "lr_grid = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation score: {}\".format(lr_grid.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(lr_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So using a minimum document frequency or removing stop words does not affect our cross-validation accuracy to a great extent, although the reduction of features might mean a lesser probability of over-fitting, as well as the ease of interpretation of a model with a lower number of features. __Note__: We do not utilize tf-idf here, since each tweet text entry is at-most 140 characters, thus the odds of one term appearing multiple times in one document are low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already discussed the usefulness of the cross-val score metric here. In our twitter early warning system, a Positive is when Bieber is in trouble, and a Negative is when the system encounters a tweet where Bieber is not in trouble. In this case, a False Positive is less harmful than a False Negative, i.e., we care more about flagging correctly all the tweets which imply that Bieber is in trouble, and if, in the process, we label a couple of harmless tweets as harmful, then, well, better safe than sorry. So, we should be looking at a high __Recall__ value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9454545454545454\n",
      "Best parameters:  {'C': 100}\n"
     ]
    }
   ],
   "source": [
    "# Just for clarity, this is the Count Vectorizer we are presently using\n",
    "\n",
    "text_train, y_train = train['Tweet_text'], train['Trouble?']\n",
    "text_test, y_test = test['Tweet_text'], test['Trouble?']\n",
    "\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10,100,1000]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(lr_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    153\n",
       "1     12\n",
       "Name: Trouble?, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Trouble?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[151   2]\n",
      " [  4   8]]\n"
     ]
    }
   ],
   "source": [
    "# Let's use a confusion matrix\n",
    "\n",
    "pred_lr = lr_grid.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test,pred_lr)\n",
    "\n",
    "print(\"Confusion matrix \\n {}\".format(conf_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have 4 false negatives, out of a sample of 12. This could indicate a problem with the model, or a lack of adequate data. (See Section 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Not in Trouble       0.97      0.99      0.98       153\n",
      "    In Trouble       0.80      0.67      0.73        12\n",
      "\n",
      "   avg / total       0.96      0.96      0.96       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Do a classification report\n",
    "\n",
    "print(classification_report(y_test,pred_lr,target_names=[\"Not in Trouble\",\"In Trouble\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, the recall for \"In Trouble\" is 0.67. We should look to improve this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and address the class imbalance in our dataset, by upsampling the minority class, with replacement, setting the two classes to have an equal number of entries in the training set. __Note__ We do this only for the train set, and not for the complete dataset, since then some of the upsampled tweet text entries would occur in both the train and test sets, so we would be evaluating our model's generalization performance on data that it will have already seen, making it useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[:385]\n",
    "test = df[385:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    347\n",
       "1     38\n",
       "Name: Trouble?, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Trouble?'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Upsample the train set\n",
    "train_majority = train[train['Trouble?']==0]\n",
    "train_minority = train[train['Trouble?']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_minority_upsampled = resample(train_minority, \n",
    "                                 replace=True,     \n",
    "                                 n_samples=347,    \n",
    "                                 random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    347\n",
       "0    347\n",
       "Name: Trouble?, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_balanced = pd.concat([train_majority,train_minority_upsampled])\n",
    "train_balanced['Trouble?'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a balanced training set. Let us evaluate the performance of our Logistic Regression classifier on this again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9956772334293948\n",
      "Best parameters:  {'C': 100}\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = train_balanced['Tweet_text'], train_balanced['Trouble?']\n",
    "text_test, y_test = test['Tweet_text'], test['Trouble?']\n",
    "\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10,100,1000]}\n",
    "lr_grid = GridSearchCV(LogisticRegression(), param_grid, cv=10)\n",
    "lr_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {}\".format(lr_grid.best_score_))\n",
    "print(\"Best parameters: \", lr_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(lr_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[150   3]\n",
      " [  3   9]]\n"
     ]
    }
   ],
   "source": [
    "pred_lr = lr_grid.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test,pred_lr)\n",
    "\n",
    "print(\"Confusion matrix \\n {}\".format(conf_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Not in Trouble       0.98      0.98      0.98       153\n",
      "    In Trouble       0.75      0.75      0.75        12\n",
      "\n",
      "   avg / total       0.96      0.96      0.96       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred_lr,target_names=[\"Not in Trouble\",\"In Trouble\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we see a slight improvement in the Recall for the Positive class (\"In Trouble\") if we use balanced classes for training our model. In reality, this translates to just 1 less False Negative, given how small our dataset is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us also try a penalized SVM classifier and see if we can improve our Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validation score: 0.9506493506493506\n",
      "Best parameters:  {'C': 1000, 'gamma': 0.001}\n"
     ]
    }
   ],
   "source": [
    "text_train, y_train = train['Tweet_text'], train['Trouble?']\n",
    "text_test, y_test = test['Tweet_text'], test['Trouble?']\n",
    "\n",
    "vect = CountVectorizer().fit(text_train)\n",
    "X_train = vect.transform(text_train)\n",
    "\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10,100,1000],'gamma':[0.001, 0.01, 0.1, 1, 10,100,1000]}\n",
    "svc_grid = GridSearchCV(SVC(class_weight='balanced'), param_grid, cv=10)\n",
    "svc_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation score: {}\".format(svc_grid.best_score_))\n",
    "print(\"Best parameters: \", svc_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.9636363636363636\n"
     ]
    }
   ],
   "source": [
    "X_test = vect.transform(text_test)\n",
    "print(\"Score: {}\".format(svc_grid.score(X_test,y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix \n",
      " [[150   3]\n",
      " [  3   9]]\n"
     ]
    }
   ],
   "source": [
    "pred_svc = svc_grid.predict(X_test)\n",
    "conf_mat = confusion_matrix(y_test,pred_svc)\n",
    "\n",
    "print(\"Confusion matrix \\n {}\".format(conf_mat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "Not in Trouble       0.98      0.98      0.98       153\n",
      "    In Trouble       0.75      0.75      0.75        12\n",
      "\n",
      "   avg / total       0.96      0.96      0.96       165\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,pred_lr,target_names=[\"Not in Trouble\",\"In Trouble\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So a penalized Support Vector Classifier on the imbalanced training set gives us the same generalization performance (And Recall for the Positive class) as a Logistic Regression Classifier applied to a balanced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. What next for the Bieber? -- Comments and further development"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i. __Most Importantly__ :: We need more data. The dataset I used was hampered by being imbalanced, and more importantly, by not having enough entries. To improve the generalization performance of the model, and to make it an effective early warning system, one would need a lot more training (and hold-out) data. This can easily be accomplished given the tools are already in place. A simple code to continuously pull tweets using different query keywords from the Twitter API should suffice. This process can be made more efficient as well, by not pulling retweets if the original tweet has already been extracted.\n",
    "\n",
    "### ii. The number of samples can also be artificially generated using Markov chain based methods (https://github.com/jsvine/markovify). (See below). Techniques like SMOTE can also be used to generate synthetic data. (http://contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.over_sampling.SMOTE.html).\n",
    "\n",
    "### iii. I have just used a preprocessor for cleaning tweets, but a second-check using regular expressions is required.\n",
    "\n",
    "### iv. Feature Engineering has not been applied to this project yet. There is usually a case to be made for punctuations such as an exclamation mark signifying something of importance; however, due to the nature of random tweets pulled from the Twitter stream, associating exclamation marks with something exclusively related to negative news about Justin Bieber seems like a recipe for overfitting.\n",
    "\n",
    "### v. In the two Machine Learning models that have been used, the parameter space has not yet been explored fully. \n",
    "\n",
    "### vi. Other Machine Learning models, especially tree-based ones such as Random Forest classifiers did not, on a cursory exploration, improve the model generalization performance. However, the grid search using these classifiers is yet to be fully investigated. \n",
    "\n",
    "## Overall:: This was a fun project, though it still needs some work to be considered a viable Twitter early warning system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix. Becoming the Bieber -- Using Markov chains to generate new tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used articles on the internet to compile two txt documents. One is general news, and news about Justin Bieber in which he is not associated with anything negative. The other document contains information from articles which report on something negative about Justin Bieber. I then ran the two texts through __Markovify__ to generate a set of 140 character texts, which I will proceed to classify as Positive (Bieber in trouble) or Negative (Bieber not in trouble). There is scope for ambiguity here, but if enough synthetic tweets are generated, it should ensure more data for our training set. Examples of this are shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  The Biebs showed up for a new electric blue Lamborghini.\n",
      "\n",
      "\n",
      "\n",
      "1 :  The Antonov An-148 was en route to the airport, presumably with plans to fly to New York City.\n",
      "\n",
      "\n",
      "\n",
      "2 :  Earlier that same day, the Sorry singer was spotted leaving a recording studio in Los Angeles on Friday night.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import markovify\n",
    "\n",
    "# Bieber not in trouble -- text\n",
    "with open(\"/Users/panchamb/Documents/Data_Analysis/GumGum/bieber_trouble_false.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "    # Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "for i in range(3):\n",
    "    print(i,\": \",text_model.make_short_sentence(140))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  During the trip, he was in in Barcelona, Bieber reportedly shared a photo of Kerr in a room with the law again.\n",
      "\n",
      "\n",
      "\n",
      "1 :  Justin Bieber accidentally ran over a sofa to get his arm through the car window.\n",
      "\n",
      "\n",
      "\n",
      "2 :  He has also been accused of arrogance; though, to be associated with someone who beats women, great!\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Bieber in trouble -- text\n",
    "with open(\"/Users/panchamb/Documents/Data_Analysis/GumGum/bieber_trouble_true.txt\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Build the model.\n",
    "text_model = markovify.Text(text)\n",
    "\n",
    "for i in range(3):\n",
    "    print(i,\": \",text_model.make_short_sentence(140))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
